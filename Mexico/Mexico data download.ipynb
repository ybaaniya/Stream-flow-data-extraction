import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Function to download the CSV file
def download_csv(url, folder):
    # Get the file name from the URL
    filename = url.split("/")[-1]
    # Full path to save the file
    filepath = os.path.join(folder, filename)

    # Download the CSV
    response = requests.get(url)
    with open(filepath, 'wb') as file:
        file.write(response.content)
    print(f"Downloaded: {filename}")

# Function to scrape the webpage and find .csv links
def download_csv_links_from_page(webpage_url, download_folder):
    # Ensure the download folder exists
    if not os.path.exists(download_folder):
        os.makedirs(download_folder)

    # Get the content of the webpage
    response = requests.get(webpage_url)
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find all <a> tags with href that end in .csv
    csv_links = [urljoin(webpage_url, a['href']) for a in soup.find_all('a', href=True) if a['href'].endswith('.csv')]

    # Download each CSV link found
    for link in csv_links:
        download_csv(link, download_folder)


# List of webpage URLs
webpage_urls = [
    'https://sih.conagua.gob.mx/climas.html',
    'https://sih.conagua.gob.mx/hidros.html'
]

download_folder = 'Path/to/save/the/csv/files'

# Process each webpage URL
for webpage_url in webpage_urls:
    print(f"Processing: {webpage_url}")
    download_csv_links_from_page(webpage_url, download_folder)
