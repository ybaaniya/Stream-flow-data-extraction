{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mexico = 'path to the mexico metadata csv file'\n",
    "csv_folder = 'Path to the folder containing the individual CSV files'\n",
    "output_folder = 'path to a folder to save the processed CSV files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each row in the filtered DataFrame\n",
    "for index, row in df_mexico.iterrows():\n",
    "    identifier = row['samplingFeatureCode']\n",
    "    csv_file_path = os.path.join(csv_folder, f'{identifier}.csv')\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(csv_file_path):\n",
    "        # Read the CSV file, skipping the first 7 rows, and setting the next row as header\n",
    "        df = pd.read_csv(csv_file_path, skiprows=7, header=0)\n",
    "\n",
    "        # Display current columns for debugging purposes\n",
    "        print(f\"Processing file: {csv_file_path}\")\n",
    "        print(f\"Columns before renaming: {df.columns.tolist()}\")\n",
    "\n",
    "        # Rename the first column to 'Datetime'\n",
    "        df.rename(columns={df.columns[0]: 'Datetime'}, inplace=True)\n",
    "\n",
    "        # Drop the second column\n",
    "        df.drop(columns=[df.columns[1]], inplace=True)\n",
    "\n",
    "        # Rename the third column to 'Streamflow (m3/s)'\n",
    "        df.rename(columns={df.columns[1]: 'Streamflow (m3/s)'}, inplace=True)\n",
    "\n",
    "        # Replace '-' with -9999 in 'Streamflow (m3/s)' column\n",
    "        df['Streamflow (m3/s)'] = df['Streamflow (m3/s)'].replace('-', -9999)\n",
    "\n",
    "        # Drop rows where 'Streamflow (m3/s)' is NaN or missing\n",
    "        df.dropna(subset=['Streamflow (m3/s)'], inplace=True)\n",
    "\n",
    "        # Include only rows between the first and last available data\n",
    "        # Identify the first and last valid rows (where 'Streamflow (m3/s)' is not -9999)\n",
    "        valid_rows = df[df['Streamflow (m3/s)'] != -9999]\n",
    "        if not valid_rows.empty:\n",
    "            first_index = valid_rows.index[0]\n",
    "            last_index = valid_rows.index[-1]\n",
    "            df = df.loc[first_index:last_index]\n",
    "\n",
    "        # Save the processed file to the output folder\n",
    "        output_file_path = os.path.join(output_folder, f'{identifier}.csv')\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "        print(f'Processed and saved: {output_file_path}')\n",
    "    else:\n",
    "        print(f'File not found for identifier: {identifier}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the data as written in the gauge documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply zero-replacement logic\n",
    "def replace_zeros(streamflow):\n",
    "    modified_streamflow = streamflow.copy()\n",
    "    \n",
    "    # Case 1: Zeros at the start\n",
    "    if modified_streamflow[0] == 0:\n",
    "        i = 0\n",
    "        # Replace all leading zeros with -9999\n",
    "        while i < len(modified_streamflow) and modified_streamflow[i] == 0:\n",
    "            modified_streamflow[i] = -9999\n",
    "            i += 1\n",
    "    \n",
    "    # Now, let's check for individual or consecutive zeros in the remaining data\n",
    "    for i in range(1, len(modified_streamflow)):\n",
    "        if modified_streamflow[i] == 0:\n",
    "            # Start of a zero sequence, check the previous value\n",
    "            prev_value = modified_streamflow[i-1]\n",
    "            zero_start = i\n",
    "            \n",
    "            # Find end of consecutive zero sequence\n",
    "            while i < len(modified_streamflow) and modified_streamflow[i] == 0:\n",
    "                i += 1\n",
    "            zero_end = i  # End of the zero sequence (exclusive)\n",
    "            \n",
    "            # If the previous day's value is < 1, keep zeros\n",
    "            if prev_value < 1:\n",
    "                continue\n",
    "            else:\n",
    "                # Replace all zeros in the sequence with -9999\n",
    "                modified_streamflow[zero_start:zero_end] = -9999\n",
    "\n",
    "    return modified_streamflow\n",
    "\n",
    "# Folder paths\n",
    "input_folder_path = '/Users/yubin/Library/CloudStorage/Box-Box/Bias Correction/mexico/Hydroserver/Unprocessed'  # Folder where your 1st level processed CSV files are stored\n",
    "output_folder_path = '/Users/yubin/Library/CloudStorage/Box-Box/Bias Correction/mexico/Hydroserver/Processed'  # Folder where you want to save modified files\n",
    "summary_file_path = os.path.join('/Users/yubin/Library/CloudStorage/Box-Box/Bias Correction/mexico/Hydroserver/zero_summary.csv')  # Summary CSV file path\n",
    "\n",
    "# Ensure the output folder exists, if not, create it\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# List to store summary information\n",
    "summary_data = []\n",
    "\n",
    "# Loop through each file in the input folder\n",
    "for filename in os.listdir(input_folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_folder_path, filename)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert Datetime column if necessary\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        \n",
    "        # Count the number of zeros before replacement\n",
    "        initial_zero_count = (df['Streamflow (m3/s)'] == 0).sum()\n",
    "        \n",
    "        # Apply the replacement logic\n",
    "        df['Modified_Streamflow'] = replace_zeros(df['Streamflow (m3/s)'])\n",
    "        \n",
    "        # Count the number of zeros after replacement\n",
    "        final_zero_count = (df['Modified_Streamflow'] == 0).sum()\n",
    "        \n",
    "        # Save the modified data back to the output folder\n",
    "        modified_file_path = os.path.join(output_folder_path, filename.replace(\".csv\", \"_modified.csv\"))\n",
    "        df.to_csv(modified_file_path, index=False)\n",
    "        \n",
    "        # Append summary information to the list\n",
    "        summary_data.append({\n",
    "            'File': filename,\n",
    "            'Initial_Zeros': initial_zero_count,\n",
    "            'Final_Zeros': final_zero_count\n",
    "        })\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Save the summary to a CSV file in the output folder\n",
    "summary_df.to_csv(summary_file_path, index=False)\n",
    "\n",
    "print(f\"Summary file saved to: {summary_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary calculation of the mexico data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = '/Users/yubin/Library/CloudStorage/Box-Box/Bias Correction/mexico/Hydroserver/Processed'\n",
    "output_path = '/Users/yubin/Library/CloudStorage/Box-Box/Bias Correction/mexico/Hydroserver/'\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Define the range of years and months\n",
    "years = list(range(1940, 2024 + 1))\n",
    "months = list(range(1, 13))\n",
    "\n",
    "# Create empty DataFrames for yearly and monthly summaries with the required columns\n",
    "yearly_summary_df = pd.DataFrame(columns=['File'] + years)\n",
    "monthly_summary_df = pd.DataFrame(columns=['File'] + months)\n",
    "\n",
    "# Loop over each CSV file to process data\n",
    "for file in csv_files:\n",
    "    # Extract the file name without the .csv extension\n",
    "    file_name = os.path.basename(file).replace('.csv', '')\n",
    "    \n",
    "    # Read the CSV file\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    # Convert the 'Datetime' column to datetime type\n",
    "    data['Datetime'] = pd.to_datetime(data['Datetime'])\n",
    "    \n",
    "    # Extract year and month from the 'Datetime' column\n",
    "    data['Year'] = data['Datetime'].dt.year\n",
    "    data['Month'] = data['Datetime'].dt.month\n",
    "    \n",
    "    # Remove non-numerical values, convert to numeric and remove NaN\n",
    "    data['Modified_Streamflow'] = pd.to_numeric(data['Modified_Streamflow'], errors='coerce')\n",
    "    \n",
    "    # Filter out rows where the value is negative, zero, or NaN\n",
    "    valid_data = data[(data['Modified_Streamflow'] > 0) & (~data['Modified_Streamflow'].isna())]\n",
    "    \n",
    "    # Group by year and count the number of valid (non-negative, non-zero) observations\n",
    "    yearly_counts = valid_data.groupby('Year').size().reindex(years, fill_value=0).reset_index(name='Valid Observations')\n",
    "    yearly_counts = yearly_counts.set_index('Year').transpose()\n",
    "    yearly_counts['File'] = file_name\n",
    "    yearly_summary_df = pd.concat([yearly_summary_df, yearly_counts], ignore_index=True)\n",
    "    \n",
    "    # Group by month and count the number of valid (non-negative, non-zero) observations across all years\n",
    "    monthly_counts = valid_data.groupby('Month').size().reindex(months, fill_value=0).reset_index(name='Valid Observations')\n",
    "    monthly_counts = monthly_counts.set_index('Month').transpose()\n",
    "    monthly_counts['File'] = file_name\n",
    "    monthly_summary_df = pd.concat([monthly_summary_df, monthly_counts], ignore_index=True)\n",
    "\n",
    "# Reorder the columns to have 'File' as the first column\n",
    "yearly_summary_df = yearly_summary_df[['File'] + years]\n",
    "monthly_summary_df = monthly_summary_df[['File'] + months]\n",
    "\n",
    "# Define the paths to save the summary CSV files\n",
    "yearly_summary_file_path = os.path.join(output_path, 'yearly_summary_valid_observations.csv')\n",
    "monthly_summary_file_path = os.path.join(output_path, 'monthly_summary_valid_observations.csv')\n",
    "\n",
    "# Save the summary DataFrames to CSV files\n",
    "yearly_summary_df.to_csv(yearly_summary_file_path, index=False)\n",
    "monthly_summary_df.to_csv(monthly_summary_file_path, index=False)\n",
    "\n",
    "print(f\"Yearly summary saved to {yearly_summary_file_path}\")\n",
    "print(f\"Monthly summary saved to {monthly_summary_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
